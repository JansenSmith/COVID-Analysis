---
title: "COVID Analysis - Unveiling Chronological Periodicity"
author: "Jansen Smith"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Purpose

This is an analysis of NYPD shooting incidents occurring from the start of 2006 to the end of 2022. The data and analysis presented herein is an effort to understand and analyze the provided data and to draw some meaningful conclusions as part of the “Data Science As A Field Course” at UC Boulder in the Summer 2 session in 2023.
Due to the source of the data provided, all analysis should be considered purely conjectural.
In particular, this report seeks to establish that the NYPD's reported shooting data demonstrates periodicity by the time of year.

## Data Source and Description

The data was obtained from NYC's open data API and contains information on NYPD shooting incidents. The dataset includes columns like OCCUR_DATE (date of incident), to which columns such as Week_of_Year were added to facilitate analysis.


## Importing required libraries
The only libraries required to run the following analysis are tidyverse and lubridate, and this code block is designed to automatically install tidyverse if the user has not already done so.

```{r libraries}
# Install the tidyverse package if not already installed
if (!requireNamespace("tidyverse", quietly = TRUE)) {
  install.packages("tidyverse")
}

# Load the tidyverse package
library(tidyverse)

# Load the lubridate package for parsing dates
library(lubridate)
```

## COVID-19 data import

Begin by reading in cases and deaths data from four main csv files, over US and global scope.

```{r import}
url = "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/"
filenames = c("time_series_covid19_confirmed_global.csv",  "time_series_covid19_deaths_global.csv", "time_series_covid19_confirmed_US.csv",  "time_series_covid19_deaths_US.csv")
urls = str_c(url, filenames)

global_cases = read_csv(urls[1])
global_deaths = read_csv(urls[2])
US_cases = read_csv(urls[3])
US_deaths = read_csv(urls[4])



#data = read_csv(url)
#data = data %>%
#  mutate(OCCUR_DATE = mdy(OCCUR_DATE), Week_of_Year = lubridate::week(OCCUR_DATE)) %>%
#  select(-X_COORD_CD, -Y_COORD_CD, -Latitude, -Longitude, -Lon_Lat, -BORO, -LOC_OF_OCCUR_DESC, -PRECINCT, -JURISDICTION_CODE, -LOC_CLASSFCTN_DESC, -LOCATION_DESC, -STATISTICAL_MURDER_FLAG, -PERP_AGE_GROUP, -PERP_SEX, -PERP_RACE, OCCUR_TIME, -VIC_AGE_GROUP, -VIC_SEX, -VIC_RACE)
#spec(data)
#head(data, 20)
#summary(data)
```

## Tidy global data
After looking at global_cases and global_deaths, we would like to tidy those datasets and put each variable (date, cases, deaths) in their own column.
Also, we don't need Lat and Long for our analysis, so we will get rid of those and rename Region and State to be more R friendly.

```{r tidy_global_data}

global_cases_tidy = global_cases %>%
  rename(Province_State = "Province/State", Country_Region = "Country/Region") %>%
  select("Province_State", "Country_Region", matches("^\\d{1,2}/\\d{1,2}/\\d{2}$")) %>%
  pivot_longer(cols = -c('Province_State',
                         'Country_Region'),
               names_to = "date",
               values_to = "cases") %>%
  mutate(date = as.Date(date, format = "%m/%d/%y"))

global_deaths_tidy <- global_deaths %>%
  rename(Province_State = "Province/State", Country_Region = "Country/Region") %>%
  select("Province_State", "Country_Region", matches("^\\d{1,2}/\\d{1,2}/\\d{2}$")) %>%
  pivot_longer(cols = -c('Province_State', 'Country_Region'),
               names_to = "date",
               values_to = "deaths") %>%
  mutate(date = as.Date(date, format = "%m/%d/%y"))
```

## Tidy US data
After looking at US_cases and US_deaths, we would like to tidy those datasets and put each variable (date, cases, deaths) in their own column.
Also, we don't need Lat and Long for our analysis, so we will get rid of those and rename Region and State to be more R friendly.

``` {r tidy_US_data}
US_cases_tidy = US_cases %>%
  pivot_longer(cols = -(UID:Combined_Key),
               names_to = "date",
               values_to = "cases") %>%
  mutate(date = as.Date(date, format = "%m/%d/%y")) %>%
  select(Admin2:cases) %>%
  select(-c(Lat, Long_))


US_deaths_tidy = US_deaths %>%
  pivot_longer(cols = -(UID:Combined_Key),
               names_to = "date",
               values_to = "deaths") %>%
  mutate(date = as.Date(date, format = "%m/%d/%y")) %>%
  select(Admin2:deaths) %>%
  select(-c(Lat, Long_))

```

## Join datasets
Merge datasets to allow for cases and deaths to be separate variables in the same dataset.
``` {r merge_data}
global = global_cases_tidy %>%
  full_join(global_deaths_tidy) %>%
  filter(cases > 0)
summary(global)

US = US_cases_tidy %>%
  full_join(US_deaths_tidy)
summary(US)

```
# Convert UID to character and exclude incompatible columns
global_cases <- global_cases %>%
    mutate(UID = as.character(UID)) %>%
    select(-c(iso2, code3, FIPS, Admin2, iso3, Lat, Long_))  # Exclude columns you don't need

# Gather the data into a longer format
global_cases <- global_cases %>%
    pivot_longer(
        cols = starts_with("1/"),  # Select columns corresponding to dates
        names_to = "date",
        values_to = "cases",
        values_drop_na = TRUE  # Drop NA values
    ) %>%
    mutate(date = as.Date(date, format = "%m/%d/%y"))

# Tidy the data by pivoting longer
global_cases <- global_cases %>%
    pivot_longer(
        cols = c(Province_State, "Country_Region"),
        names_to = "location",
        values_to = "value"
    ) %>%
    arrange(location, date)

global <- global_cases %>%
    full_join(global_deaths)

# Display the resulting tidy dataset
print(global_cases)


```

## NYPD Shooting data visualization

```{r plotting}
data %>%
  ggplot(aes(x = OCCUR_DATE)) +
  geom_line(stat = "count") +
  labs(title = "Number of NYPD Shooting Incidents Over Time",
       x = "Date",
       y = "Number of Incidents") +
  theme_minimal() 

# Plot the number of incidents per week of the year, aggregating all years
data %>%
  group_by(Week_of_Year) %>%
  summarize(Num_Incidents = n(), Median_Incidents = median(Num_Incidents)) %>%
  ggplot(aes(x = Week_of_Year, y = Num_Incidents)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_line(aes(y = Median_Incidents), color = "red", linewidth = 1.5) +  # Add a red line for the median
  labs(title = "Aggregate Number of NYPD Shooting Incidents per Week of the Year",
       x = "Week of the Year",
       y = "Number of Incidents") +
  theme_minimal()
```

## Analysis

- As we can see from the shooting incidents over time, the data shows yearly periodicity.
- When aggregated by week, this periodicity can be seen more clearly, with significantly more incidents occurring during the summer months.
- Based on these findings, there is a potential argument for the NYPD to reevaluate its approach towards public safety and community support during the winter months, considering the demonstrated slowdown in shooting activity during that time.

## Bias Assessment

- The format of the analysis was specifically chosen to avoid known sources of bias, with the notable exception of inherent bias due to trusting NYPD incident reporting sources.
- Relying on the NYPD's self-reported statistics to establish groundtruth crime data is itself inherently flawed. For example, the NYPD is likely to diminish the prelevance of crimes by the NYPD itself.
- My hope is that chronological periodicity is a topic that would be cross-cultural and thus mitigate the impacts of my own cultural bias.
- Political bias, however, should not be discounted. It is possible that the data suggests patterns that the author's mistrust of the NYPD may leave uncovered.